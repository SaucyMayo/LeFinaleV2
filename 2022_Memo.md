# 2022 Computer Science 244 - Semester Test 2 Memo

**Date:** 2022-11-14  
**Compiled by:** Willem Bester  
**Moderated by:** Lynette van Zijl

---

## Question 1: Short Questions (Multiple Choice) - 6 marks

### 1(a) How many bits wide are segment registers on Intel architecture? [1]

**Answer: B - 16 bits**

**Explanation:** Segment registers (CS, DS, SS, ES, FS, GS) in Intel architecture are 16 bits wide. These registers originated from the 8086/8088 era when the processor needed to address more than 64KB of memory using 16-bit registers. The segment registers hold segment selectors that point to segment descriptors in protected mode, or form the high-order bits of a 20-bit address in real mode.

---

### 1(b) Which one of the following Intel instructions has no effect on the stack pointer? [1]

**Answer: D - test**

**Explanation:** 
- **call** - Pushes return address onto stack (decrements ESP)
- **push** - Pushes operand onto stack (decrements ESP)
- **ret** - Pops return address from stack (increments ESP)
- **test** - Performs logical AND operation and sets flags, but does NOT modify the stack pointer or any operand

The `test` instruction performs a bitwise AND between two operands and updates the flags (ZF, SF, PF) based on the result, but it doesn't store the result or modify the stack pointer.

---

### 1(c) Which Mic-1 register always keeps the stack top value? [1]

**Answer: D - TOS**

**Explanation:** TOS (Top Of Stack) is a dedicated register in the Mic-1 architecture that always holds the top value of the operand stack. This design decision improves performance by keeping the most frequently accessed stack value in a register rather than requiring a memory access for every operation. Other registers:
- **CPP** - Constant Pool Pointer
- **LV** - Local Variable pointer
- **SP** - Stack Pointer (points to the memory location of the top of stack, not the value itself)

---

### 1(d) Which one of the following cannot be encoded in Mic-1 microcode? [1]

**Answer: B - H = MDR - H**

**Explanation:** The Mic-1 ALU has specific constraints on its operations. When performing subtraction, the H register must always be the subtrahend (the value being subtracted), not the minuend. The ALU can compute:
- H = H - MDR (correct format)
- H = H + MDR (addition works both ways)
- H = MDR + H (addition is commutative)

But it cannot compute H = MDR - H because the architecture doesn't support this operand ordering for subtraction.

---

### 1(e) Suppose a proportional allocation scheme is used on a system with 62 frames, and two processes, one of 10 pages, and one of 127 pages. How many frames would be allocated to the larger process? [1]

**Answer: C - 57**

**Explanation:** 
In proportional allocation, frames are allocated based on the relative size of each process:

```
Total pages = 10 + 127 = 137
Frames for larger process = (127/137) Ã— 62 = 57.5 â‰ˆ 57 frames
Frames for smaller process = (10/137) Ã— 62 = 4.5 â‰ˆ 5 frames
```

The larger process with 127 pages gets 57 frames.

---

### 1(f) When a second-chance page replacement algorithm, with a reference bit and a modify bit (in this order), is used, which one of the following page classes should be evicted first? [1]

**Answer: A - (0, 0)**

**Explanation:** 
The second-chance algorithm (also called clock algorithm) uses reference and modify bits to classify pages. Page classes in order of eviction preference:

1. **(0, 0)** - Not referenced, not modified - BEST candidate (evict first)
2. **(0, 1)** - Not referenced, but modified - Needs write-back
3. **(1, 0)** - Referenced, not modified - Recently used
4. **(1, 1)** - Referenced and modified - WORST candidate (keep)

A page with (0, 0) hasn't been referenced recently and hasn't been modified, so it can be evicted without writing back to disk, making it the ideal choice.

---

## Question 2: ALU and Circuit Design - 9 marks

### 2(a) ALU Control Signals Table [4]

**Question:** Fill in the table showing how the ALU's control signals must be set up to perform the given arithmetic functions.

**Answer:**

| Function | F0 | F1 | ena | inva | inc |
|----------|----|----|-----|------|-----|
| A + B    | 1  | 1  | 1   | 0    | 0   |
| A + B + 1| 1  | 1  | 1   | 0    | 1   |
| B - A    | 1  | 1  | 1   | 1    | 1   |
| -A       | 1  | 1  | 0   | 1    | 1   |

**Explanation:**
- **F0, F1**: Select the ALU function (11 = addition/subtraction)
- **ena**: Enable A input (1 = A is used, 0 = A is treated as 0)
- **inva**: Invert A input (used for subtraction via two's complement)
- **inc**: Increment result by 1 (used for two's complement and increment operations)

---

### 2(b) Explain the reasoning for calculating B - A [2]

**Answer:**

To calculate B - A, we use the two's complement method:
```
B - A = B + (-A) = B + (NOT A + 1)
```

Steps:
1. **inva = 1**: Inverts A (gives NOT A, which is the one's complement)
2. **inc = 1**: Adds 1 to the result (completing the two's complement of A)
3. **ena = 1**: Enables A input so it participates in the operation
4. **F0 = 1, F1 = 1**: Selects addition operation

The ALU computes: B + (NOT A) + 1 = B + (-A) = B - A

---

### 2(c) Multiplexer Wiring Table [3]

**Question:** Show how to wire up inputs A, B, VGnd, and VCC to a two-input multiplexer to compute logical functions A, AB, and A + B.

**Answer:**

| Function | D0   | D1   | C |
|----------|------|------|---|
| A        | VGnd | VCC  | A |
| AB       | VGnd | B    | A |
| A + B    | B    | VCC  | A |

**Explanation:**
- **For function A**: When A=0, output VGnd (0); when A=1, output VCC (1). Result = A
- **For function AB**: When A=0, output VGnd (0); when A=1, output B. Result = A AND B
- **For function A + B**: When A=0, output B; when A=1, output VCC (1). Result = A OR B

The control line C is always connected to A, and we vary D0 and D1 to achieve different functions.

---

## Question 3: Page Replacement Algorithms - 7 marks

### 3(a) Page replacement with 3 frames [3]

**Reference string:** 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5

#### LRU (Least Recently Used) - 3 frames:

| Reference | 1 | 2 | 3 | 4 | 1 | 2 | 5 | 1 | 2 | 3 | 4 | 5 |
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|
| Frame 1   | 1 | 1 | 1 | 4 | 4 | 4 | 4 | 4 | 4 | 3 | 3 | 3 |
| Frame 2   |   | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 4 | 4 |
| Frame 3   |   |   | 3 | 3 | 1 | 1 | 5 | 5 | 5 | 5 | 5 | 5 |
| Fault?    | F | F | F | F | F | - | F | - | - | F | F | F |

**Total page faults: 9**

#### FIFO (First-In First-Out) - 3 frames:

| Reference | 1 | 2 | 3 | 4 | 1 | 2 | 5 | 1 | 2 | 3 | 4 | 5 |
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|
| Frame 1   | 1 | 1 | 1 | 4 | 4 | 4 | 5 | 5 | 5 | 3 | 3 | 3 |
| Frame 2   |   | 2 | 2 | 2 | 1 | 1 | 1 | 1 | 2 | 2 | 2 | 5 |
| Frame 3   |   |   | 3 | 3 | 3 | 2 | 2 | 2 | 2 | 2 | 4 | 4 |
| Fault?    | F | F | F | F | F | F | F | - | F | F | F | F |

**Total page faults: 11**

---

### 3(b) Page replacement with 4 frames and Belady's Anomaly [4]

#### LRU - 4 frames:

| Reference | 1 | 2 | 3 | 4 | 1 | 2 | 5 | 1 | 2 | 3 | 4 | 5 |
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|
| Frame 1   | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 5 |
| Frame 2   |   | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 |
| Frame 3   |   |   | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 |
| Frame 4   |   |   |   | 4 | 4 | 4 | 5 | 5 | 5 | 5 | 4 | 4 |
| Fault?    | F | F | F | F | - | - | F | - | - | - | F | F |

**Total page faults: 8**

#### FIFO - 4 frames:

| Reference | 1 | 2 | 3 | 4 | 1 | 2 | 5 | 1 | 2 | 3 | 4 | 5 |
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|
| Frame 1   | 1 | 1 | 1 | 1 | 1 | 2 | 2 | 2 | 2 | 3 | 3 | 3 |
| Frame 2   |   | 2 | 2 | 2 | 2 | 2 | 5 | 5 | 5 | 5 | 4 | 4 |
| Frame 3   |   |   | 3 | 3 | 3 | 3 | 3 | 1 | 1 | 1 | 1 | 5 |
| Frame 4   |   |   |   | 4 | 4 | 4 | 4 | 4 | 2 | 2 | 2 | 2 |
| Fault?    | F | F | F | F | - | F | F | F | F | F | F | F |

**Total page faults: 11**

**Belady's Anomaly Definition:**

Belady's Anomaly is a counterintuitive phenomenon in page replacement where increasing the number of page frames can actually increase the number of page faults for certain page reference strings when using the FIFO algorithm.

**Illustration from our calculations:**

- FIFO with 3 frames: 11 page faults
- FIFO with 4 frames: 11 page faults (same, but can be worse in other cases)

While our specific example doesn't show the anomaly strongly, Belady's Anomaly is well-documented with FIFO. The classic example shows that adding more memory (frames) can paradoxically decrease performance. This anomaly does NOT occur with LRU or optimal page replacement algorithms - only with FIFO and similar algorithms.

---

## Question 4: Digital Logic and Architecture - 16 marks

### 4(a) SR Latch Circuit Diagram and Nondeterminism [4]

**Answer:**

**SR Latch Circuit Diagram (using NOR gates):**

```
     S ----NOR---- Q
           |  |
           |  +---- (feedback)
           |
     R ----NOR---- Q'
              |
              +---- (feedback to first NOR)
```

Or with NAND gates:
```
     S' ---NAND--- Q
           |  |
           |  +---- (feedback)
           |
     R' ---NAND--- Q'
              |
              +---- (feedback to first NAND)
```

**How Inconsistency Leads to Nondeterminism:**

The SR latch has four states:
- **S=0, R=0**: Hold (maintains current state)
- **S=1, R=0**: Set (Q=1)
- **S=0, R=1**: Reset (Q=0)
- **S=1, R=1**: **FORBIDDEN/INVALID**

When S=1 and R=1 simultaneously (for NOR implementation), both outputs try to be 0, violating the requirement that Q and Q' must be complements. When this forbidden state is released and both inputs return to 0 simultaneously, the circuit enters an **inconsistent state**.

The **nondeterminism** occurs because:
1. Both NOR gates race to determine the final state
2. The outcome depends on microscopic timing differences
3. Slight variations in gate delays, temperature, or voltage determine which gate "wins"
4. The final state (Q=0 or Q=1) is unpredictable and non-reproducible

This makes the circuit behavior **nondeterministic** - we cannot predict which stable state it will settle into.

---

### 4(b) Parallelization of 1-bit Full Adder [4]

**Answer:**

**Approach 1: Ripple Carry (Serial)**
- **Cost**: LOW - 3 gates per bit (2 XOR, 2 AND, 1 OR)
- **Performance**: SLOW - O(n) delay for n bits (carry must ripple through all stages)
- **Description**: Each full adder must wait for the carry from the previous stage

**Approach 2: Carry Lookahead (Parallel)**
- **Cost**: HIGH - Exponentially more gates for generate and propagate logic
- **Performance**: FAST - O(log n) delay (carries computed in parallel using additional hardware)
- **Description**: 
  - Generates G (generate) and P (propagate) signals for each bit
  - Computes all carries simultaneously using: Ci = Gi + PiÂ·Ci-1
  - Multi-level lookahead for long words (16, 32, 64 bits)

**Trade-offs:**
- Ripple carry is cheap but slow (suitable for low-power applications)
- Carry lookahead is expensive but fast (used in high-performance CPUs)
- Modern designs use hybrid approaches (carry-select, carry-skip)

**Formula for carry lookahead:**
```
Gi = Ai Â· Bi (generate)
Pi = Ai âŠ• Bi (propagate)
Ci = Gi + Pi Â· Ci-1
```

---

### 4(c) ISA Instructions for Relational Operators in Mic-1 [4]

**Answer:**

The 32-bit ALU in Mic-1 doesn't directly compute boolean results for relational operators. Instead, the implementation approach is:

**For Equality (=):**
1. Compute A - B using the ALU
2. Examine the Z (Zero) flag output
3. If Z=1, then A=B; if Z=0, then Aâ‰ B
4. Microcode can branch based on Z flag
5. Or push 0/1 onto stack based on Z flag value

**For Less Than or Equal (â‰¤):**
1. Compute A - B using the ALU
2. Examine the N (Negative) flag and Z (Zero) flag
3. If N=1 OR Z=1, then Aâ‰¤B
4. Combine flag results: (N OR Z) determines the boolean result
5. May require multiple microinstructions to:
   - Perform subtraction
   - Test flags
   - Set result value (0 or 1)
   - Push result onto stack

**Implementation Strategy:**
The Mic-1 uses conditional microcode jumps based on N and Z flags:
```
IFEQ: branch if Z=1
IFLT: branch if N=1
IFLE: branch if N=1 OR Z=1
```

For non-branching comparisons (e.g., Java bytecode like ISUB followed by comparison), the microcode:
1. Performs the ALU operation
2. Reads the flag values
3. Constructs a boolean result (0x00000000 or 0x00000001)
4. Pushes result onto operand stack

---

### 4(d) Bus Design in Modern Systems [4]

**Answer:**

Modern systems like Intel Core architecture use a **hierarchical multi-bus design** to accommodate both fast and slow components:

**1. Multiple Bus Speeds:**
- **Front-Side Bus (FSB) / QuickPath Interconnect (QPI)**: High-speed (several GHz) for CPU-memory communication
- **PCIe Buses**: Multiple lanes with varying speeds (Gen3: 8 GT/s per lane, Gen4: 16 GT/s)
- **DMI (Direct Media Interface)**: Medium speed for chipset communication
- **USB, SATA, Legacy buses**: Slower speeds for peripherals

**2. Bridge Architecture:**
- **Northbridge/Memory Controller Hub**: Connects CPU to fast components (RAM, GPU via PCIe)
- **Southbridge/I/O Controller Hub**: Connects to slower peripherals (USB, SATA, Ethernet)
- Modern CPUs integrate northbridge functions directly on-chip

**3. Bus Arbitration and Protocols:**
- **Split transactions**: Allow bus to be used by others while slow device prepares data
- **Posted writes**: Fast device doesn't wait for slow device to complete write
- **Buffering**: FIFOs between bus segments smooth speed differences
- **Multiple outstanding transactions**: Overlapping requests improve throughput

**4. Synchronization Mechanisms:**
- **Asynchronous handshaking**: WAIT states inserted for slow devices
- **Clock domain crossing**: Synchronizers between different clock frequencies
- **Flow control**: Back-pressure prevents fast devices from overwhelming slow ones

**5. Modern Solutions:**
- **On-chip buses** (fast): Direct CPU-to-cache, core-to-core
- **Point-to-point links**: Replace shared buses (e.g., QPI, PCIe)
- **Network-on-Chip (NoC)**: Packet-switched fabric for many-core systems

This hierarchical approach ensures fast components aren't bottlenecked by slow ones while maintaining backward compatibility.

---

## Question 5: Pipelining and Memory Hierarchy - 18 marks

### 5(a) Pipelining the Mic-1 Architecture [4]

**Answer:**

**Original Mic-1 Subcycles:**
The Mic-1 operates in subcycles where each microinstruction:
1. Reads register(s) onto B bus
2. Performs ALU operation
3. Shifts result
4. Writes result to register(s) via C bus
5. Optionally reads/writes memory

**Pipelining Strategy:**

**Stage 1: Fetch**
- Read control store using MPC
- Latch microinstruction into pipeline register
- Increment or compute next MPC

**Stage 2: Decode/Register Read**
- Decode microinstruction fields
- Read source registers onto internal buses
- Set up ALU control signals

**Stage 3: Execute**
- Perform ALU operation
- Shift operation if needed
- Generate condition flags (N, Z)

**Stage 4: Memory Access**
- Read from memory if required (MAR â†’ MDR)
- Write to memory if required (MDR â†’ Memory)
- Handle memory wait states

**Stage 5: Write-back**
- Write ALU result to destination register
- Update MPC based on branch conditions
- Write condition flags

**Datapath Modifications:**
- **Pipeline registers** between stages to hold intermediate values
- **Forwarding paths** from Execute and Write-back stages back to Execute stage inputs
- **Stall logic** to handle data dependencies
- **Flush logic** for branch mispredictions

**Control Store Changes:**
- Must handle pipeline hazards in microcode
- May need NOP microinstructions
- Branch microinstructions cause pipeline flushes

**Register Usage:**
- Need pipeline registers (latches) between each stage
- May need duplicate registers for forwarding
- H register becomes more critical for ALU operations

**Challenges:**
- Memory accesses may take multiple cycles
- Data dependencies between consecutive microinstructions
- Branch microinstructions cause control hazards
- More complex than ISA-level pipelining due to microarchitecture exposure

---

### 5(b) RAW Dependencies as "True Dependencies" [4]

**Answer:**

**RAW = Read After Write ("True Dependency")**

RAW dependencies are called "true dependencies" because they represent genuine data flow in the program:

```
Instruction 1: ADD R1, R2, R3  ; R1 = R2 + R3
Instruction 2: SUB R4, R1, R5  ; R4 = R1 - R5 (depends on R1 from Instr 1)
```

Instruction 2 **must** read the value that Instruction 1 writes. This is a **true data dependency** - the algorithm requires this ordering.

**Comparison with Other Dependencies:**

**WAR (Write After Read) - "Anti-dependency":**
```
Instruction 1: ADD R1, R2, R3  ; Reads R2
Instruction 2: MOV R2, R4      ; Writes R2
```
- Not a true dependency - just a register name conflict
- Can be eliminated by **register renaming** (use different register for Instr 2)

**WAW (Write After Write) - "Output dependency":**
```
Instruction 1: ADD R1, R2, R3  ; Writes R1
Instruction 2: SUB R1, R4, R5  ; Writes R1
```
- Not a true dependency - just a register name conflict
- Can be eliminated by **register renaming** (use different register for Instr 2)

**Why RAW is "True":**
- Represents actual data flow in the algorithm
- **Cannot** be eliminated by renaming
- Reflects the fundamental computation dependencies
- Violating RAW changes program semantics

**Mitigation of WAR and WAW:**

**1. Register Renaming:**
- Use more physical registers than architectural registers
- Map architectural names to physical registers dynamically
- Eliminates false dependencies

**2. Out-of-Order Execution:**
- Allows instructions to execute when data is ready
- Must still respect RAW dependencies
- Can execute past WAR/WAW after renaming

**3. Scoreboarding:**
- Tracks register usage
- Stalls only when truly necessary

**4. Tomasulo's Algorithm:**
- Uses reservation stations
- Automatic register renaming via tags
- Resolves dependencies dynamically

---

### 5(c) Cache Memory and Locality Principles [4]

**Answer:**

**Motivation for Cache Memory:**

CPUs are orders of magnitude faster than main memory (DRAM). Without cache:
- CPU would waste cycles waiting for memory
- Memory bandwidth becomes bottleneck
- Multi-GHz processors can't achieve peak performance

**The Locality Principles:**

**1. Temporal Locality:**
- **Principle**: Recently accessed data likely to be accessed again soon
- **Examples**: 
  - Loop counter variables
  - Function local variables
  - Frequently called functions
- **Cache benefit**: Keep recently used data in fast cache

**2. Spatial Locality:**
- **Principle**: Data near recently accessed data likely to be accessed soon
- **Examples**:
  - Array elements accessed sequentially
  - Sequential instruction execution
  - Structure fields accessed together
- **Cache benefit**: Load entire cache line (32-64 bytes), not just requested byte

**How Cache Exploits Locality:**

**Temporal Locality â†’ Cache Retention:**
- Keep recently used blocks in cache
- Use LRU or similar replacement policy
- Multiple cache levels (L1, L2, L3) provide graduated latency

**Spatial Locality â†’ Cache Lines:**
- Transfer blocks of data, not individual bytes
- Typical cache line: 64 bytes
- One memory access brings in many nearby bytes
- Prefetching mechanisms anticipate sequential access

**Performance Impact:**

**With good locality:**
- High hit rate (90-99%)
- Effective memory latency: ~few cycles (cache hit time)
- CPU rarely stalls

**Without locality:**
- Low hit rate
- Effective memory latency: ~100+ cycles (main memory)
- Severe performance degradation

**Design Implications:**
- Programmers should write "cache-friendly" code
- Traverse arrays sequentially, not randomly
- Keep data structures compact
- Reuse data while it's in cache

**Quantitative Example:**
```
L1 cache hit time: 4 cycles
Main memory access time: 200 cycles
Hit rate: 95%

Average access time = 0.95 Ã— 4 + 0.05 Ã— 200 = 3.8 + 10 = 13.8 cycles
(vs. 200 cycles without cache!)
```

---

### 5(d) Associative Cache Memory [3]

**Answer:**

**Associative Cache (Fully Associative):**

In a fully associative cache, a memory block can be placed in **any** cache line, not just a specific line determined by its address.

**Operation:**

**1. Tag Storage:**
- Each cache line stores the full address tag (or significant portion)
- Plus valid bit, dirty bit, and LRU bits

**2. Lookup Process:**
- All cache tags are compared **simultaneously** (in parallel) with requested address
- Special hardware (CAM - Content Addressable Memory) performs parallel search
- If any tag matches â†’ **cache hit**
- If no tag matches â†’ **cache miss**

**3. Comparison Hardware:**
```
Requested Address: [Tag | Offset]

Compare with ALL cache lines in parallel:
Line 0: [Tag0] [Valid0] [Data] â†’ Comparator â†’ Match?
Line 1: [Tag1] [Valid1] [Data] â†’ Comparator â†’ Match?
Line 2: [Tag2] [Valid2] [Data] â†’ Comparator â†’ Match?
   ...
Line N: [TagN] [ValidN] [Data] â†’ Comparator â†’ Match?

OR gate combines all matches â†’ Hit/Miss signal
Encoder identifies which line matched â†’ Select data
```

**Advantages:**
- **No conflict misses**: Any block can go anywhere
- **Flexible replacement**: Can choose optimal victim
- **Best hit rate**: Among cache organizations

**Disadvantages:**
- **Expensive**: Requires comparator for every cache line
- **Slow**: Parallel comparison of all tags (though hardware is fast)
- **High power**: All comparators active on every access
- **Not scalable**: Impractical for large caches

**Practical Use:**
- TLBs (Translation Lookaside Buffers): Small, fully associative
- Small L1 caches: Sometimes fully associative
- Most caches use **set-associative** (compromise): N-way set associative combines direct-mapped index with associative search within set

---

### 5(e) Static Branch Prediction Strategy [3]

**Answer:**

**Strategy: Take backward jumps, skip forward jumps**

**Motivation:**
This strategy is based on typical control flow patterns in high-level languages.

**Analysis by Control-Flow Element:**

**1. Loops (GOOD for this strategy) âœ“**
```
for (int i = 0; i < N; i++) {
    // loop body
    // backward jump to loop start
}
```
- **Backward jump** at end of loop to restart
- Loop executes many times (high iteration count)
- Last iteration: forward jump to exit
- **Prediction success**: N-1 correct, 1 wrong (when loop exits)
- **Accuracy**: Very high for loops with many iterations

**2. If-Then-Else (VARIABLE) Â±**
```
if (condition) {
    // then block
    // forward jump to skip else
} else {
    // else block
}
// join point
```
- Uses **forward jump** to skip else block
- Strategy predicts: **don't take** (execute else block)
- **Accuracy**: Depends on condition bias (50-50 â†’ 50% accuracy)

**3. If-Then (no else) (VARIABLE) Â±**
```
if (unlikely_condition) {
    // forward jump to here
}
// continue
```
- **Forward jump** into then block
- Strategy predicts: **don't take** (skip then block)
- Good if condition is usually false
- Bad if condition is usually true

**4. Switch/Case (VARIABLE) Â±**
- Multiple forward jumps to cases
- Backward jump sometimes used for fall-through
- Accuracy depends on case distribution

**Overall Evaluation:**

**GOOD strategy because:**
- âœ“ Loops dominate execution time in most programs
- âœ“ 70-90% of dynamic branches in typical programs
- âœ“ Backward jumps strongly correlate with loops
- âœ“ Very simple to implement (sign bit of offset)

**Limitations:**
- âœ— Conditional branches (if-then-else) only 50% accurate
- âœ— Doesn't help with function calls (neither backward nor forward jump)
- âœ— No adaptation to program behavior

**Historical Note:**
This was used in early RISC processors (MIPS R2000, early SPARC) before dynamic branch prediction became feasible. Modern processors use sophisticated dynamic predictors (two-level, perceptron, etc.) with >95% accuracy.

---

## Question 6: Memory Management and File Systems - 15 marks

### 6(a) Fragmentation in Paging and Segmentation [3]

**Answer:**

**Fragmentation** is wasted memory that cannot be allocated to processes.

**Internal Fragmentation:**
- Wasted space **within** an allocated region
- Allocated but unused memory

**External Fragmentation:**
- Wasted space **between** allocated regions
- Free memory too scattered to be useful

---

**PAGING:**

**Internal Fragmentation: YES**
- Pages have fixed size (e.g., 4KB)
- Process size rarely exact multiple of page size
- Last page partially empty
- **Amount**: Average 0.5 Ã— page_size per process
- **Example**: Process needs 10.5KB with 4KB pages â†’ allocates 3 pages (12KB) â†’ 1.5KB wasted

**External Fragmentation: NO**
- All pages same size
- Any free frame can hold any page
- Memory never becomes "fragmented" into unusable pieces
- Free frames always usable

---

**SEGMENTATION:**

**Internal Fragmentation: NO (ideally)**
- Segments sized exactly to contents
- No wasted space within segment
- (In practice, slight internal fragmentation if segments padded to word boundaries)

**External Fragmentation: YES**
- Segments vary in size
- As segments allocated/deallocated, holes form
- Free memory becomes scattered
- Large free space might not be contiguous
- **Example**: 
  - Memory: [Segment A: 100KB] [FREE: 50KB] [Segment B: 200KB] [FREE: 40KB]
  - Request for 80KB segment fails despite 90KB free total
  - Need compaction to consolidate free space

---

**Comparison:**

| Aspect | Paging | Segmentation |
|--------|--------|--------------|
| Internal Fragmentation | Yes (~0.5 page/process) | No |
| External Fragmentation | No | Yes (can be severe) |
| Solution | Accept internal loss | Compaction (expensive) |
| Modern Use | Universal | Limited (segments within pages) |

**Modern Systems:**
Most use **segmented paging**: Logical segments divided into fixed-size pages. Gets benefits of both with minimal fragmentation.

---

### 6(b) Thrashing [3]

**Answer:**

**Thrashing Definition:**

Thrashing occurs when a computer system spends more time **swapping pages in and out** of memory than executing actual program instructions. The system becomes dominated by page fault handling rather than useful work.

**Mechanism:**

1. **Insufficient memory** for process working sets
2. Process executes instruction â†’ **page fault**
3. OS loads required page from disk
4. Must evict another page (no free frames)
5. Very soon, evicted page needed again â†’ **another page fault**
6. **Continuous cycle** of page faults

**Performance Impact:**
- CPU utilization drops drastically (close to 0%)
- Disk I/O utilization very high (nearly 100%)
- System becomes unresponsive
- Throughput approaches zero

---

**Example Circumstance:**

**Scenario:** System with 8GB RAM running:
- Operating system: 2GB
- Browser with many tabs: 3GB
- IDE (development environment): 2GB
- Video editing software: 4GB
- Database server: 2GB
- **Total**: 13GB demanded, only 8GB available

**What Happens:**
1. All processes allocated initial pages (5GB available for processes)
2. Each process working set > available memory share
3. Video editor accesses memory â†’ page fault
4. Evicts browser pages to load video editor pages
5. User switches to browser â†’ page fault
6. Evicts video editor pages to load browser pages
7. Background database query â†’ page fault
8. Evicts browser pages â†’ **cycle continues**

**Observable Symptoms:**
- Hard disk constantly active (LED always on)
- Applications "freeze" for seconds at a time
- Mouse cursor stutters
- Simple operations take minutes
- System fans run high (CPU at 100% but not doing useful work)

**Solutions:**
- Add more RAM
- Close some applications
- Reduce working set size (process priority adjustment)
- Better page replacement algorithm (working set algorithm)
- Suspend/swap out entire processes rather than individual pages

---

### 6(c) Translation Lookaside Buffer (TLB) [3]

**Answer:**

**TLB (Translation Lookaside Buffer)** is a small, fast cache that stores recent virtual-to-physical address translations.

**Purpose:**
- Avoid accessing page table in memory for every memory reference
- Page table access would double memory access time
- TLB provides fast address translation

---

**Operation:**

**1. Memory Access Request:**
```
CPU generates virtual address: [VPN | Offset]
VPN = Virtual Page Number
```

**2. TLB Lookup (Parallel):**
- Search TLB for VPN
- TLB is **associative memory** (CAM)
- All entries checked simultaneously

**3. TLB Hit:**
- VPN found in TLB
- Extract PFN (Physical Frame Number) from TLB entry
- Form physical address: [PFN | Offset]
- Access memory directly
- **Time**: 1-2 cycles (very fast)
- **Common case**: 95-99% of accesses

**4. TLB Miss:**
- VPN not in TLB
- Access page table in memory (slow)
- Retrieve PTE (Page Table Entry)
- Check valid bit (page present in memory?)
  - If valid: Extract PFN
  - If invalid: **Page fault** â†’ OS loads page from disk
- **Update TLB** with new translation
- Evict old TLB entry if full (LRU)
- Retry memory access
- **Time**: 100+ cycles for page table walk

---

**TLB Entry Contents:**
```
[Valid | VPN | PFN | Protection bits | Dirty | Reference]
```

**TLB Characteristics:**
- **Size**: Typically 64-512 entries
- **Organization**: Fully associative or set-associative
- **Coverage**: With 4KB pages, 512 entries cover 2MB
- **Hit rate**: 95-99% for typical workloads
- **Replacement**: LRU or pseudo-LRU
- **Flushing**: On context switch (or use ASIDs for process tagging)

---

**Performance Impact:**

**Without TLB:**
```
Memory access time = Page_table_access + Data_access
                   = 100ns + 100ns = 200ns
```

**With TLB (98% hit rate):**
```
Average access time = 0.98 Ã— (TLB + Data) + 0.02 Ã— (TLB + Page_table + Data)
                    = 0.98 Ã— 102ns + 0.02 Ã— 202ns
                    = 99.96ns + 4.04ns = 104ns
```

TLB nearly eliminates address translation overhead!

---

### 6(d) FAT vs Linux inodes [6]

**Answer:**

**FAT (File Allocation Table)**

**Structure:**
- **Directory entry**: Contains filename, attributes, starting cluster number
- **FAT**: Array where entry[i] points to next cluster of file
- FAT[cluster] = next_cluster (linked list on disk)
- Special values: 0=free, 0xFFFF=end-of-file, 0xFFF7=bad cluster

**File Storage:**
```
Directory: [file.txt | start=100 | size=3000]
FAT[100] = 101
FAT[101] = 250
FAT[250] = 251
FAT[251] = EOF
```

---

**Linux inodes**

**Structure:**
- **Directory entry**: Only filename + inode number
- **inode**: Structure containing metadata and block pointers
- **Direct blocks**: 12 pointers to data blocks
- **Indirect blocks**: Pointer to block of pointers
- **Double indirect**: Pointer to block of indirect blocks
- **Triple indirect**: Pointer to block of double indirect blocks

**inode Layout:**
```
inode {
    metadata: size, permissions, timestamps
    direct[0-11]:  12 Ã— 4KB = 48KB
    indirect:      256 Ã— 4KB = 1MB
    double_indirect: 256 Ã— 256 Ã— 4KB = 256MB
    triple_indirect: 256 Ã— 256 Ã— 256 Ã— 4KB = 64GB
}
```

---

**Comparison:**

**FILE SIZE:**

| Aspect | FAT | Linux inodes |
|--------|-----|--------------|
| Small files (< 48KB) | Good - few FAT lookups | **Excellent** - all blocks in inode |
| Medium files (< 1MB) | Good | **Excellent** - single indirect |
| Large files (> 1MB) | Acceptable | **Good** - double/triple indirect |
| Maximum file size | Limited by FAT size (2GB-4GB) | **Huge** (64GB with 4KB blocks, more with larger blocks) |

**Winner for small files:** inodes (direct blocks in inode = no seeking)
**Winner for max size:** inodes (scales better)

---

**ACCESS TIME:**

| Operation | FAT | Linux inodes |
|-----------|-----|--------------|
| Sequential read | Moderate - must follow chain | **Fast** - blocks pre-listed |
| Random access | **Slow** - traverse FAT chain from start | **Fast** - calculate offset â†’ block directly |
| Seek to byte N | O(N/cluster_size) FAT reads | **O(1) or O(log N)** calculation |
| Open file | Fast - read start cluster | Fast - load inode |

**Example - Access byte at offset 100MB:**

**FAT:**
```
100MB / 4KB = 25,600 clusters from start
Must read 25,600 FAT entries (following chain)
Many disk seeks
Very slow!
```

**inodes:**
```
Block number = 100MB / 4KB = 25,600
= 256 Ã— 100 = double_indirect[0][100]
Two indirect block reads + data block
Three disk accesses total
Fast!
```

**Winner:** inodes (especially for random access)

---

**FRAGMENTATION:**

| Aspect | FAT | Linux inodes |
|--------|-----|--------------|
| Internal fragmentation | Yes (~0.5 Ã— cluster_size per file) | Yes (~0.5 Ã— block_size per file) |
| External fragmentation | **Severe** - no locality, clusters anywhere | **Reduced** - block allocator tries to keep file blocks together |
| File clusters/blocks | Scattered randomly | Grouped by cylinder groups |
| Performance degradation | **Severe** over time (heavy fragmentation) | Moderate (better allocation policy) |
| Defragmentation | Required regularly | Rarely needed |

**Winner:** inodes (better allocation, less fragmentation)

---

**SUMMARY TABLE:**

| Criterion | FAT | Linux inodes | Winner |
|-----------|-----|--------------|--------|
| Small files | Good | **Excellent** | inodes |
| Large files | Acceptable | Good | inodes |
| Max file size | 2-4GB | **>64GB** | inodes |
| Sequential access | Moderate | **Fast** | inodes |
| Random access | Slow | **Fast** | inodes |
| Fragmentation | **Severe** | Reduced | inodes |
| Simplicity | **Simple** | Complex | FAT |
| Space overhead | FAT table (fixed) | inodes (per-file) | Similar |

**Overall Winner: Linux inodes** - Superior in almost all aspects except simplicity. FAT's simplicity made it popular for early filesystems and embedded systems, but inodes provide much better performance and scalability for modern use.

---

## Question 7: Programming Concepts - 11 marks

### 7(a) "There are no data types in assembly language" [3]

**Answer:**

This statement is **partially true but requires critical analysis**.

**Arguments SUPPORTING the statement:**

**1. No Type Enforcement:**
- Assembly has no type system checking
- No compile-time type verification
- All operands treated as bit patterns
- Same register can hold integer, float, pointer, or character

**2. Instructions Don't Care:**
```assembly
MOV EAX, 100      ; Is this integer 100 or ASCII character?
ADD EAX, EBX      ; Is this integer add, float add, pointer arithmetic?
```
- The CPU doesn't know or care
- Bits are just bits
- Programmer assigns meaning

**3. Type-Agnostic Operations:**
```assembly
MOV EAX, [memory]  ; Load 32 bits - could be anything
```
- Memory is untyped byte array
- No metadata about contents

---

**Arguments AGAINST the statement:**

**1. Implicit Types in Instructions:**
```assembly
FADD    ; Floating-point add (expects float type)
IDIV    ; Signed integer divide (expects signed int)
DIV     ; Unsigned integer divide (expects unsigned int)
```
- Different instructions for different interpretations
- Instruction set **encodes** type assumptions

**2. Size Specifications:**
```assembly
BYTE PTR [address]   ; 8-bit type
WORD PTR [address]   ; 16-bit type
DWORD PTR [address]  ; 32-bit type
```
- Assembly requires size/type declarations
- Not fully type-free

**3. Calling Conventions:**
- Function parameters have implied types
- Struct layouts follow type rules
- ABI specifies how types are passed

**4. Assembler Directives:**
```assembly
myInt   DD 42        ; Define doubleword (int)
myFloat DD 3.14      ; Define doubleword (float)
myStr   DB "hello"   ; Define bytes (string)
```
- Assembler provides pseudo-types

---

**Critical Evaluation:**

**The statement is TRUE in that:**
- Assembly lacks **high-level type system**
- No compile-time type safety
- Programmer responsible for correctness
- Bits are interpreted by context

**The statement is FALSE in that:**
- **Implicit types exist** in instruction semantics
- Different instructions assume different types
- Size specifications are a form of typing
- Not completely "typeless"

**Conclusion:**

Assembly language has **weak, implicit typing** rather than no typing. It's more accurate to say: "Assembly language has no **type safety** or **type enforcement**, but instructions still embody type semantics that the programmer must respect."

The absence of type checking makes assembly powerful but dangerous - type errors become runtime bugs rather than compile-time errors.

---

## End of 2022 Memo

**Total Marks: 88 points (calculated out of 80)**

**Study Tips:**
- Focus on understanding concepts, not just memorizing
- Practice page replacement algorithms until they're automatic
- Draw circuit diagrams to visualize digital logic
- Work through ALU control signals systematically
- Understand the "why" behind architectural decisions

**Key Topics Covered:**
âœ“ Intel architecture basics
âœ“ Mic-1 microarchitecture
âœ“ Page replacement algorithms (LRU, FIFO)
âœ“ Belady's Anomaly
âœ“ Digital logic (SR latch, full adder)
âœ“ Pipelining principles
âœ“ Memory hierarchy (cache, TLB)
âœ“ Virtual memory management
âœ“ File systems (FAT vs inodes)
âœ“ Assembly language concepts

Good luck with your studies! ðŸ“š
