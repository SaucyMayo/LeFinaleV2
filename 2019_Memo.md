# 2019 Computer Science 244 - Semester Test 2 Memo

**Date:** 2019-11-04  
**Compiled by:** Willem Bester  
**Moderated by:** Willem Visser

---

## Question 1: Short Questions (Multiple Choice) - 5 marks

### 1(a) Which one of the following Intel instructions has no effect on the stack pointer? [1]

**Answer: B - cmp**

**Explanation:**
- **call** - Pushes return address onto stack (decrements ESP)
- **cmp** - Compares two operands by subtracting and setting flags, does NOT modify stack pointer or operands
- **pop** - Pops value from stack (increments ESP)
- **ret** - Pops return address from stack (increments ESP)

The `cmp` instruction performs a non-destructive comparison (source - destination) and only updates flags (ZF, SF, CF, OF, PF, AF). Neither operand nor the stack pointer is modified.

---

### 1(b) Which one of the following cannot be encoded in Mic-1 microcode? [1]

**Answer: B - H = MDR - H**

**Explanation:** The Mic-1 ALU architecture has specific constraints on subtraction operations. When performing subtraction in the form A - B, the H register must always be the subtrahend (B), not the minuend (A).

Valid operations:
- H = H - MDR âœ“ (H is subtrahend on right side of ALU)
- H = H + MDR âœ“ (addition works both ways)
- H = MDR + H âœ“ (addition is commutative)

Invalid operation:
- H = MDR - H âœ— (would require H as subtrahend on left side)

The hardware datapath doesn't support swapping operands for subtraction, making MDR - H impossible to encode.

---

### 1(c) Proportional allocation with 62 frames [1]

**Answer: C - 57**

**Calculation:**
```
Total pages = 10 + 127 = 137
Frames for process 1 = (10/137) Ã— 62 = 4.53 â‰ˆ 5 frames
Frames for process 2 = (127/137) Ã— 62 = 57.47 â‰ˆ 57 frames
Verify: 5 + 57 = 62 âœ“
```

The larger process (127 pages) receives 57 frames under proportional allocation.

---

### 1(d) Dependency type for the given instruction sequence [1]

```
R1 = R2 * R3
R4 = R5 + R1
```

**Answer: A - RAW (Read After Write)**

**Explanation:**
- Instruction 1 **writes** to R1
- Instruction 2 **reads** from R1
- This is a **Read After Write (RAW)** dependency, also called a "true dependency"

The second instruction must wait for the first to complete because it needs the value produced by the first instruction. In a pipelined processor, this creates a data hazard requiring either stalling or forwarding.

---

### 1(e) Second-chance page replacement - which page class to evict first? [1]

**Answer: A - (0, 0)**

**Explanation:**
Page classes with (reference bit, modify bit):
1. **(0, 0)** - Not referenced, not modified - **BEST** eviction candidate
2. **(0, 1)** - Not referenced, modified - Must write back
3. **(1, 0)** - Referenced, not modified - Recently used
4. **(1, 1)** - Referenced, modified - **WORST** candidate

(0, 0) is ideal because:
- Not recently used (R=0)
- Clean page, no writeback needed (M=0)
- Cheapest to evict

---

## Question 2: Definitions (One Sentence Each) - 7 marks

### 2(a) Bus skew [1]

**Answer:** Bus skew is the variation in arrival times of signals on different lines of a bus caused by different wire lengths, capacitances, or propagation delays, which can cause timing problems if the skew exceeds the system's timing margins.

---

### 2(b) Sign extension [1]

**Answer:** Sign extension is the operation of increasing the bit-width of a signed integer by replicating the sign bit (most significant bit) into the new higher-order positions to preserve the numerical value.

**Example:** Extending 8-bit -5 (11111011) to 16-bit: 1111111111111011

---

### 2(c) Unified cache memory [1]

**Answer:** Unified cache memory is a cache organization where a single cache holds both instructions and data, as opposed to split caches (Harvard architecture) where instructions and data have separate caches.

**Advantages:** Flexibility in allocation, simpler design
**Disadvantages:** Potential conflicts between instruction and data accesses

---

### 2(d) Register renaming [1]

**Answer:** Register renaming is a hardware technique that dynamically maps architectural registers to a larger set of physical registers to eliminate false dependencies (WAR and WAW hazards) and enable greater instruction-level parallelism.

**Example:** 
```
R1 = R2 + R3  â†’ P10 = P20 + P30
R1 = R4 + R5  â†’ P11 = P40 + P50 (different physical register)
```

---

### 2(e) Internal fragmentation (memory allocation) [1]

**Answer:** Internal fragmentation is the wasted memory space within an allocated memory block that occurs when the allocated block size exceeds the actual amount needed by the process, typically because allocation occurs in fixed-size units.

**Example:** Process needs 3.7KB, gets 4KB page â†’ 0.3KB wasted (internal fragmentation)

---

### 2(f) External fragmentation (memory allocation) [1]

**Answer:** External fragmentation is the wasted memory that exists as small scattered free blocks between allocated blocks, where the total free memory is sufficient for a request but cannot be allocated because no single contiguous block is large enough.

**Example:** 100KB free in 50 separate 2KB holes â†’ cannot allocate 60KB request

---

### 2(g) Copy-on-write [1]

**Answer:** Copy-on-write is a memory optimization technique where multiple processes share the same physical memory pages marked read-only until one process attempts to modify a page, at which point a private copy is created for that process.

**Use case:** `fork()` system call - child process initially shares parent's memory

---

## Question 3: Page Replacement Calculations - 9 marks

### 3(a) Page replacement with 3 frames [4]

**Reference string:** 3, 4, 1, 5, 3, 4, 2, 3, 4, 1, 5, 2

#### LRU (Least Recently Used) - 3 frames:

| Reference | 3 | 4 | 1 | 5 | 3 | 4 | 2 | 3 | 4 | 1 | 5 | 2 |
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|
| Frame 1   | 3 | 3 | 3 | 5 | 5 | 5 | 5 | 5 | 5 | 1 | 1 | 1 |
| Frame 2   |   | 4 | 4 | 4 | 3 | 3 | 3 | 3 | 3 | 3 | 5 | 5 |
| Frame 3   |   |   | 1 | 1 | 1 | 4 | 2 | 2 | 4 | 4 | 4 | 2 |
| Fault?    | F | F | F | F | F | F | F | - | F | F | F | F |

**Total page faults: 11**

#### FIFO (First-In First-Out) - 3 frames:

| Reference | 3 | 4 | 1 | 5 | 3 | 4 | 2 | 3 | 4 | 1 | 5 | 2 |
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|
| Frame 1   | 3 | 3 | 3 | 5 | 5 | 5 | 2 | 2 | 2 | 1 | 1 | 1 |
| Frame 2   |   | 4 | 4 | 4 | 3 | 3 | 3 | 3 | 3 | 3 | 5 | 5 |
| Frame 3   |   |   | 1 | 1 | 1 | 4 | 4 | 4 | 4 | 4 | 4 | 2 |
| Fault?    | F | F | F | F | F | F | F | - | - | F | F | F |

**Total page faults: 10**

---

### 3(b) Page replacement with 4 frames and Belady's Anomaly [5]

#### LRU - 4 frames:

| Reference | 3 | 4 | 1 | 5 | 3 | 4 | 2 | 3 | 4 | 1 | 5 | 2 |
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|
| Frame 1   | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 2 |
| Frame 2   |   | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 |
| Frame 3   |   |   | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 5 | 5 |
| Frame 4   |   |   |   | 5 | 5 | 5 | 2 | 2 | 2 | 2 | 2 | 2 |
| Fault?    | F | F | F | F | - | - | F | - | - | - | F | F |

**Total page faults: 8**

#### FIFO - 4 frames:

| Reference | 3 | 4 | 1 | 5 | 3 | 4 | 2 | 3 | 4 | 1 | 5 | 2 |
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|
| Frame 1   | 3 | 3 | 3 | 3 | 3 | 4 | 4 | 4 | 4 | 1 | 1 | 1 |
| Frame 2   |   | 4 | 4 | 4 | 4 | 4 | 2 | 2 | 2 | 2 | 5 | 5 |
| Frame 3   |   |   | 1 | 1 | 1 | 1 | 1 | 3 | 3 | 3 | 3 | 2 |
| Frame 4   |   |   |   | 5 | 5 | 5 | 5 | 5 | 4 | 4 | 4 | 4 |
| Fault?    | F | F | F | F | - | F | F | F | F | F | F | F |

**Total page faults: 11**

---

**Belady's Anomaly Definition:**

Belady's Anomaly is a counterintuitive phenomenon in page replacement algorithms where **increasing the number of available page frames can actually result in more page faults** for certain reference strings. This anomaly occurs with FIFO and some other page replacement algorithms, but **not** with stack-based algorithms like LRU or Optimal.

**Illustration from our calculations:**

| Algorithm | 3 Frames | 4 Frames | Observation |
|-----------|----------|----------|-------------|
| LRU       | 11 faults | 8 faults | Improved (normal) âœ“ |
| FIFO      | 10 faults | 11 faults | **Worse! Belady's Anomaly!** |

For FIFO, adding a frame (from 3 to 4) **increased** page faults from 10 to 11. This demonstrates Belady's Anomaly - more memory resulted in worse performance! This paradoxical behavior occurs because FIFO doesn't consider recency of use, and the different eviction patterns with 4 frames happen to be worse for this specific reference string.

**Why LRU doesn't exhibit the anomaly:**
LRU is a "stack algorithm" - the set of pages in memory with n frames is always a subset of pages in memory with n+1 frames. This property guarantees that more frames never hurts performance.

---

## Question 4: Mic-1 Architecture and Performance - 24 marks

### 4(a) Four subcycles of Mic-1 data path cycle [4]

**Answer:**

The Mic-1 microarchitecture executes each microinstruction in four distinct subcycles:

**Subcycle 1: Memory Fetch/Store (if needed)**
- If memory read required: Address from MAR presented to memory, data fetched into MDR
- If memory write required: Address from MAR and data from MDR written to memory
- Memory operations take full subcycle due to slow memory access
- If no memory operation: This subcycle still occurs but does nothing

**Subcycle 2: Register Selection and B Bus Drive**
- Control signals select which register(s) to read
- Selected register's contents driven onto B bus
- All registers connected to B bus via tri-state buffers
- Only one register enabled at a time to avoid bus conflicts

**Subcycle 3: ALU Operation and Shifting**
- ALU receives inputs from B bus and H register
- ALU performs specified operation (ADD, AND, pass A, pass B, etc.)
- ALU control signals (F0, F1, ENA, ENBA, INVA, INC) determine function
- Result passes through shifter (SLL8, SRA1, or no shift)
- N and Z condition codes generated based on ALU output

**Subcycle 4: C Bus Write-Back**
- Shifter output driven onto C bus
- Control signals select destination register(s)
- Multiple registers can be written simultaneously from C bus
- MAR and MDR may be loaded for next cycle's memory operation
- MPC (microprogramcounter) updated for next microinstruction (may be incremental, jump to NEXT_ADDRESS, or conditional branch based on N/Z)

**Timing:** All subcycles complete in one clock cycle. The clock period must be long enough to accommodate the slowest operation (typically memory access in subcycle 1).

---

### 4(b) Branching to goto vs T/F - evaluation of proposal [3]

**Question:** Since the first micro-operation at T is the same as the first micro-operation for goto, can all branching instructions simply branch to either goto or F instead of T or F?

**Answer: This is a BAD idea and would NOT work correctly.**

**Why the proposal fails:**

**1. Conditional vs Unconditional Semantics:**
- **goto** is UNCONDITIONAL - always proceeds to fetch next instruction
- **T branch** is CONDITIONAL - reaches T only when condition is TRUE
- The microcode at goto assumes it's always executed
- The microcode at T may have condition-specific setup

**2. Z flag handling:**
- goto instruction explicitly checks Z flag to decide PC increment
- T branch destination has already evaluated the condition
- Reusing goto's Z-check would be redundant and incorrect

**3. MPC (Micro Program Counter) state:**
- goto expects to be entered from main decode loop
- T expects to be entered from conditional test
- Internal state and assumptions differ

**4. Microcode flow control:**
```
Conditional branch:
  IF condition THEN goto T ELSE goto F
  T: [specific handling for true case]
  F: [specific handling for false case]
  Both eventually reach common code

goto:
  Unconditional jump to next instruction fetch
```

**5. Practical problem:**
- If IFEQ (if equal/zero) branches to goto on Z=1:
  - goto checks Z again (but why? it's already 1)
  - goto increments PC (incorrect for equal case!)
  - loses track of whether comparison was equal or not

**Correct design:**
Conditional branches need separate T and F paths to:
- Handle true/false cases differently
- Maintain proper control flow
- Keep microcode clear and maintainable
- Support different ISA instruction semantics

**Conclusion:** While the proposal might save one microinstruction location in control store, it would break the semantics of conditional branching and make the microcode incorrect and unmaintainable.

---

### 4(c) Two strategies for increasing data path execution speed [4]

**Answer:**

**Strategy 1: Increase Clock Frequency**

**Method:**
- Reduce clock period (increase frequency)
- Requires faster components (ALU, registers, memory)
- May need to pipeline memory accesses

**Advantages:**
- Straightforward approach
- Benefits all operations
- No change to ISA or microcode

**Disadvantages:**
- Limited by slowest component (usually memory)
- Increased power consumption and heat
- More expensive components needed
- Eventually hits physical limits (wire delays, heat dissipation)

**Practical considerations:**
- Can use cache to speed up memory accesses
- May need wait states for slow operations
- Asynchronous memory interfaces help

---

**Strategy 2: Pipelining the Data Path**

**Method:**
- Divide data path into stages (fetch, decode, execute, writeback)
- Insert pipeline registers between stages
- Multiple microinstructions in flight simultaneously
- Throughput increased even though latency per instruction unchanged

**Implementation:**
```
Stage 1: Control store fetch
Stage 2: Register read / ALU setup
Stage 3: ALU operation / Shift
Stage 4: Register writeback
```

**Advantages:**
- Can execute 4 microinstructions simultaneously (4Ã— throughput)
- Doesn't require faster components
- Clock frequency can remain same or even decrease slightly

**Disadvantages:**
- Pipeline hazards must be handled:
  - Data hazards (RAW dependencies) â†’ forwarding or stalling
  - Control hazards (branches) â†’ prediction or flushing
- Increased hardware complexity (forwarding paths, hazard detection)
- Some performance lost to stalls
- More complex microcode (must avoid/handle hazards)

**Practical performance:**
- Ideal: 4Ã— speedup with 4 stages
- Reality: 2-3Ã— speedup due to hazards
- Still significant improvement

---

**Additional Strategies (brief mentions):**

**Strategy 3: Wider Data Paths**
- Use 64-bit or 128-bit ALU instead of 32-bit
- Process more data per cycle
- Requires significant redesign

**Strategy 4: Multiple Functional Units**
- Parallel ALUs, separate load/store unit
- Superscalar execution
- Complex control logic

**Conclusion:** Pipelining typically provides the best performance improvement for reasonable complexity increase, which is why modern processors use deep pipelines (10-20+ stages).

---

### 4(d) RAW dependencies as "true dependencies" [4]

*[Same content as 2022 memo - included for completeness]*

**Answer:**

**RAW = Read After Write ("True Dependency")**

RAW dependencies are called "true dependencies" because they represent genuine data flow in the program:

```
Instruction 1: ADD R1, R2, R3  ; R1 = R2 + R3
Instruction 2: SUB R4, R1, R5  ; R4 = R1 - R5 (depends on R1 from Instr 1)
```

Instruction 2 **must** read the value that Instruction 1 writes. This is a **true data dependency** - the algorithm requires this ordering.

**Comparison with Other Dependencies:**

**WAR (Write After Read) - "Anti-dependency":**
```
Instruction 1: ADD R1, R2, R3  ; Reads R2
Instruction 2: MOV R2, R4      ; Writes R2
```
- Not a true dependency - just a register name conflict
- Can be eliminated by **register renaming**

**WAW (Write After Write) - "Output dependency":**
```
Instruction 1: ADD R1, R2, R3  ; Writes R1
Instruction 2: SUB R1, R4, R5  ; Writes R1
```
- Not a true dependency - just a register name conflict
- Can be eliminated by **register renaming**

**Why RAW is "True":**
- Represents actual data flow in the algorithm
- **Cannot** be eliminated by renaming
- Reflects fundamental computation dependencies
- Violating RAW changes program semantics

**Mitigation of WAR and WAW:**

**1. Register Renaming:**
- Hardware dynamically maps architectural to physical registers
- Eliminates false dependencies
- Enables out-of-order execution

**2. Compiler Optimizations:**
- Software register allocation
- Instruction scheduling to minimize hazards

**3. Forwarding/Bypassing:**
- For RAW dependencies that cannot be eliminated
- Pass result directly from EX stage to next instruction
- Avoids waiting for writeback

---

### 4(e) Cache memory and locality principles [3]

*[Same content as 2022 memo]*

**Motivation for Cache:**
CPUs are orders of magnitude faster than main memory. Cache bridges the speed gap.

**Temporal Locality:**
- Recently accessed data likely accessed again soon
- Examples: loop variables, function locals
- Cache keeps recently used data

**Spatial Locality:**
- Nearby data likely accessed together
- Examples: array elements, sequential instructions
- Cache loads entire cache lines (64 bytes)

**Performance Impact:**
```
With 95% hit rate, 4-cycle cache, 200-cycle memory:
Average = 0.95Ã—4 + 0.05Ã—200 = 13.8 cycles
(vs 200 cycles without cache!)
```

---

### 4(f) Associative cache memory - definition and merits [3]

**Definition:**
Associative (fully associative) cache allows any memory block to be placed in any cache line, with all tags compared in parallel using Content Addressable Memory (CAM) to find matches.

**Merits:**

**1. No Conflict Misses:**
- Any block can go anywhere
- No forced evictions due to address mapping
- Optimal cache utilization

**2. Flexible Replacement:**
- Can choose true LRU victim
- Not constrained by set membership
- Best possible hit rate for given cache size

**3. Simple Addressing:**
- No index calculation needed
- Just tag comparison

**4. Ideal for Small Caches:**
- TLBs (64-512 entries) often fully associative
- Small L1 caches may use full associativity

**Demerits:**

**1. High Cost:**
- Comparator needed for every cache line
- Expensive CAM hardware
- N comparators for N-line cache

**2. High Power:**
- All comparators active on every access
- Significant power consumption

**3. Doesn't Scale:**
- Impractical for large caches (1MB+)
- Comparison time increases with size

**Practical Compromise:**
N-way set-associative cache: Direct-mapped index to find set, then associative search within small set (typically 2, 4, 8, or 16 ways).

---

### 4(g) Static branch prediction: backward taken, forward not taken [3]

*[Same analysis as 2022 memo]*

**Strategy:** Take backward jumps, skip forward jumps

**Evaluation:**

**GOOD for Loops (backward jumps):** âœ“
- Loops dominate execution (90%+ of branches)
- Backward jump taken N-1 times, not taken once
- Very high accuracy

**VARIABLE for If-Then-Else (forward jumps):** Â±
- Depends on condition probability
- ~50% accuracy if conditions balanced

**Overall:** **Good strategy** because loops dominate, and backward jumps strongly correlate with loops. Simple to implement (check sign bit of branch offset). Typical accuracy: 60-70% overall.

**Modern alternative:** Dynamic predictors (branch history tables, two-level predictors) achieve >95% accuracy.

---

## Question 5: Memory Management and File Systems - 20 marks

### 5(a) Segmentation vs Paging - conceptual differences and faults [4]

**Answer:**

**Conceptual Differences:**

**PAGING:**
- **Concept:** Divide address space into fixed-size pages (typically 4KB)
- **Visibility:** Invisible to programmer (managed by OS/hardware)
- **Organization:** Linear address space split arbitrarily
- **Protection:** Page-level permissions
- **Sharing:** Difficult (share entire pages)
- **Growing:** No natural growth (allocate more pages)

**SEGMENTATION:**
- **Concept:** Divide address space into logical segments (code, data, stack, heap)
- **Visibility:** Visible to programmer (segment registers, segment numbers)
- **Organization:** Meaningful logical units
- **Protection:** Segment-level permissions (natural protection boundaries)
- **Sharing:** Easy (share code segment between processes)
- **Growing:** Segments can grow dynamically (stack, heap)

**Address Formation:**

**Paging:** Linear address â†’ [Page Number | Offset]
**Segmentation:** Logical address â†’ [Segment Number | Offset within segment]

---

**Differences in Faults (Programmer Perspective):**

**PAGE FAULT:**
- **Cause:** Accessing a page not in physical memory
- **Transparency:** Completely transparent to program
- **Handling:** OS loads page from disk, program automatically resumes
- **Programmer impact:** None - just slows execution
- **Recovery:** Always recoverable
- **Example:** Access array element on swapped-out page

**SEGMENTATION FAULT:**
- **Cause:** Illegal memory access violating segment bounds or permissions
- **Transparency:** Visible to program (process terminated)
- **Handling:** OS sends SIGSEGV signal to process
- **Programmer impact:** Program bug - must fix code
- **Recovery:** Usually fatal (unless handler installed)
- **Examples:**
  - Dereferencing NULL pointer
  - Buffer overflow beyond segment
  - Writing to read-only code segment
  - Accessing unmapped segment

**Key Distinction:**
- **Page fault** = Legitimate access to valid but absent page â†’ Recoverable
- **Segmentation fault** = Illegal access violating protection â†’ Programming error

**Modern Systems:**
Use **segmented paging** (x86) or pure paging (x86-64 long mode). "Segmentation fault" is misnomer on pure paging systems - really means "protection violation" or "invalid memory access".

---

### 5(b) Demand paging - definition and comparison with cache [3]

**Answer:**

**Demand Paging Definition:**

Demand paging is a virtual memory management scheme where pages are loaded into physical memory only when they are accessed (on demand), rather than loading the entire program into memory at process startup.

**Operation:**
1. Process starts with no pages in memory (or just initial page)
2. Instruction access â†’ page fault (page not present)
3. OS loads required page from disk into RAM
4. Update page table entry (mark valid)
5. Resume execution
6. Subsequent accesses to that page: no fault (in memory)
7. If memory full, evict a page using replacement algorithm

**Benefits:**
- Programs larger than physical RAM can execute
- Faster startup (don't load entire program)
- More processes can be in memory simultaneously
- Pages never accessed are never loaded

---

**Comparison with Cache Memory:**

**SIMILARITIES:**

| Aspect | Demand Paging | Cache |
|--------|---------------|-------|
| **Purpose** | Bridge speed gap | Bridge speed gap |
| **Hierarchy** | Disk â†” RAM | RAM â†” CPU |
| **Miss handling** | Page fault â†’ load from disk | Cache miss â†’ load from RAM |
| **Transparency** | Transparent to program | Transparent to program |
| **Locality** | Exploits temporal/spatial locality | Exploits temporal/spatial locality |
| **Block size** | Page (4KB) | Cache line (64B) |
| **Replacement** | LRU, FIFO, Clock, etc. | LRU, Random, etc. |

Both implement a **cache** of slower storage in faster storage.

---

**DIFFERENCES:**

| Aspect | Demand Paging | Cache |
|--------|---------------|-------|
| **Speed ratio** | 100,000Ã— (disk vs RAM) | 100Ã— (RAM vs CPU) |
| **Miss cost** | ~5-10 ms (disk seek) | ~100 ns (RAM access) |
| **Management** | Software (OS) | Hardware |
| **Block size** | Large (4KB) | Small (64B) |
| **Visibility** | OS manages explicitly | Completely transparent |
| **Replacement** | Elaborate algorithms (second chance) | Simple (LRU approximation) |
| **Write policy** | Usually write-back (dirty bit) | Varies (write-through or write-back) |
| **Miss handling** | Trap to OS (expensive) | Hardware stall (cheap) |

**Key Difference:** Demand paging is **software-managed and extremely slow on miss**, while cache is **hardware-managed and relatively fast on miss**. The enormous speed gap between disk and RAM makes paging much more performance-critical than cache misses.

---

### 5(c) Thrashing - definition and example [3]

*[Same content as 2022 memo]*

**Definition:**
Thrashing occurs when a system spends more time swapping pages in and out of memory than executing useful instructions.

**Mechanism:**
1. Insufficient memory for working sets
2. Page fault â†’ load page â†’ evict another page
3. Evicted page needed immediately â†’ another fault
4. Continuous cycle of page faults
5. CPU utilization approaches 0%, disk at 100%

**Example:**
System with 8GB RAM running:
- OS: 2GB
- Browser: 3GB
- IDE: 2GB
- Video editor: 4GB
- Database: 2GB
- **Total: 13GB demanded, only 8GB available**

Each application accesses memory â†’ page fault â†’ evicts other app's pages â†’ that app needs pages â†’ continuous swapping â†’ system becomes unresponsive.

**Solutions:** Add RAM, close applications, better scheduling, working set algorithm.

---

### 5(d) Translation Lookaside Buffer (TLB) operation [3]

*[Same content as 2022 memo]*

**TLB Definition:**
Small, fast associative cache storing recent virtual-to-physical address translations.

**Operation:**

**1. Address Translation Request:**
CPU generates virtual address [VPN | Offset]

**2. TLB Lookup (parallel):**
- Search TLB for VPN using CAM
- All entries checked simultaneously

**3. TLB Hit (95-99% of time):**
- Extract PFN from TLB entry
- Form physical address: [PFN | Offset]
- Access memory (~2 cycles total)

**4. TLB Miss:**
- Access page table in memory (~100 cycles)
- Check if page valid/present
- If invalid â†’ page fault (OS loads from disk)
- Update TLB with translation
- Retry access

**Performance Impact:**
Without TLB: Every memory access requires 2 accesses (page table + data) = 200ns
With TLB (98% hit): Average = 102ns
TLB nearly eliminates translation overhead!

---

### 5(e) Why Unix doesn't allow hard links to directories [3]

**Answer:**

Unix file systems prohibit hard links to directories (except for "." and ".." created by the system) for several critical reasons:

**1. Prevention of Cycles in Directory Structure:**

**The Problem:**
```
/home/user/A/ â†’ hardlink to /home/user/A/B/
/home/user/A/B/ â†’ hardlink to /home/user/
```

This creates a **cycle** in the directory graph. Without cycles prevention:
- **Infinite loops:** Programs like `find`, `du`, or `ls -R` would recurse infinitely
- **Unreachable files:** Files might exist but be unreachable from root
- **Can't determine path:** Multiple valid paths to same directory

**2. Reference Counting Becomes Impossible:**

**Directory Deletion:**
- Unix uses reference counting (link count) to determine when to delete files
- When link count reaches 0, file is deleted
- With cycles, directories could have references but be unreachable from root
- **Circular references:** Directory A references B, B references A â†’ both have link count â‰¥ 1, neither can be deleted (memory leak)

**Example:**
```
mkdir /tmp/A
mkdir /tmp/B
ln /tmp/B /tmp/A/linkB    # If allowed
ln /tmp/A /tmp/B/linkA    # If allowed
rm -rf /tmp/A
rm -rf /tmp/B
# Both directories still exist, unreachable, can't be deleted!
```

**3. Tree Structure Requirement:**

**File System Integrity:**
- Unix assumes directory structure is a **tree** (or DAG for symbolic links)
- Tree property: Exactly one path from root to any node
- Hard links to directories break tree property
- Tools assume tree structure (fsck, backup utilities)

**4. ".." Parent Directory Problem:**

**Ambiguity:**
- Each directory has ".." link to parent
- If directory has multiple hard links, which is the parent?
- ".." would be ambiguous or incorrect
- Path traversal (pwd, realpath) would break

**Example:**
```
/home/alice/docs â†’ /shared/documents (hardlink)
/home/bob/files â†’ /shared/documents (hardlink)

In /shared/documents, what does ".." point to?
- /home/alice? 
- /home/bob?
- /shared?
```

---

**Why Symbolic Links Are Allowed:**

Symbolic links to directories are permitted because:
- They're not counted in reference count
- Easily detected and skipped (prevents infinite loops)
- Don't affect directory deletion logic
- Clearly marked as "not real" directory entries

**Historical Note:**
Early Unix systems (V7) allowed hard links to directories for superuser only. This was removed due to the problems above. Modern systems enforce the prohibition strictly.

---

### 5(f) Unix inode strategy for large file ranges and rapid access [4]

**Answer:**

Unix file systems use a **multi-level indexing strategy** in the inode structure, providing efficient access for both small and large files without wasting space.

**inode Structure:**

```
inode {
    // Metadata
    mode, owner, size, timestamps, etc.
    
    // Block pointers (typical implementation)
    direct[12]           // Direct pointers to data blocks
    indirect             // Pointer to indirect block
    double_indirect      // Pointer to double indirect block
    triple_indirect      // Pointer to triple indirect block
}
```

---

**Multi-Level Strategy Details:**

**Level 1: Direct Blocks (12 pointers)**
- **Pointers:** 12 direct pointers to data blocks
- **Block size:** Typically 4KB
- **Capacity:** 12 Ã— 4KB = 48KB
- **Access time:** Immediate (pointers in inode)
- **Use case:** Small files

**Level 2: Single Indirect**
- **Pointer:** Points to block containing 1024 pointers (for 4KB blocks with 4-byte pointers)
- **Capacity:** 1024 Ã— 4KB = 4MB
- **Access time:** 2 disk accesses (indirect block + data block)
- **Use case:** Medium files

**Level 3: Double Indirect**
- **Pointer:** Points to block of 1024 indirect block pointers
- **Capacity:** 1024 Ã— 1024 Ã— 4KB = 4GB
- **Access time:** 3 disk accesses
- **Use case:** Large files

**Level 4: Triple Indirect**
- **Pointer:** Points to block of 1024 double indirect block pointers
- **Capacity:** 1024 Ã— 1024 Ã— 1024 Ã— 4KB = 4TB
- **Access time:** 4 disk accesses
- **Use case:** Huge files

---

**Total Capacity:**

```
Direct:          12 Ã— 4KB           = 48 KB
Indirect:        1K Ã— 4KB           = 4 MB
Double indirect: 1K Ã— 1K Ã— 4KB      = 4 GB
Triple indirect: 1K Ã— 1K Ã— 1K Ã— 4KB = 4 TB
--------------------------------------------
Total maximum file size              â‰ˆ 4 TB (with 4KB blocks)
```

With larger blocks (e.g., 16KB), capacity grows to 64TB+.

---

**Why This Strategy Works:**

**1. Space Efficiency:**
- **Small files:** Only use direct pointers (no wasted indirect blocks)
- **Medium files:** Use single indirect (minimal overhead)
- **Large files:** Pay for indirection only when needed
- inode size remains constant (~256 bytes)

**2. Rapid Access:**
- **Small files:** Immediate access (no indirection)
- **Random access:** Calculate block number â†’ traverse index blocks
- **Sequential access:** Prefetch indirect blocks

**3. Large File Range:**
- Supports files from 0 bytes to terabytes
- Graceful scaling with file size

**4. Access Time Calculation:**

For random access to byte offset N:
```
Block number B = N / 4096

If B < 12:           // Direct
    1 disk access
Else if B < 12 + 1K: // Indirect
    2 disk accesses
Else if B < 12 + 1K + 1M: // Double indirect
    3 disk accesses
Else:                // Triple indirect
    4 disk accesses
```

**Example:** Access byte at offset 100MB:
```
Block = 100MB / 4KB = 25,600
= 256 Ã— 100
â†’ double_indirect[0][100]
â†’ 3 disk accesses
```

Much better than FAT's linked list requiring 25,600 sequential accesses!

---

**Optimizations:**

**1. Caching:**
- Indirect blocks cached in buffer cache
- Frequently accessed files have cached indices
- Reduces effective access time

**2. Block Groups (ext2/3/4):**
- Related blocks stored nearby
- Reduces seek time
- Improves sequential access

**3. Extents (modern systems):**
- ext4 uses extents: [start block, length]
- More efficient for large contiguous files
- Still falls back to traditional inodes when needed

---

**Comparison with Alternatives:**

| Strategy | Small Files | Large Files | Max Size | Random Access |
|----------|-------------|-------------|----------|---------------|
| FAT (linked list) | Good | Poor | 4GB | **Very Poor** (O(n)) |
| Unix inode | **Excellent** | Good | 4TB+ | **Excellent** (O(1)-O(log n)) |
| Extent-based | Excellent | **Excellent** | 16TB+ | **Excellent** |

**Conclusion:** The multi-level inode strategy elegantly balances space efficiency for small files with support for large files and rapid random access, making it ideal for general-purpose file systems.

---

## End of 2019 Memo

**Total Marks: 89 points (calculated out of 80)**

**Key Topics Covered:**
âœ“ Intel instruction set and stack operations
âœ“ Mic-1 microarchitecture (subcycles, branching, optimization)
âœ“ Page replacement algorithms (LRU, FIFO)
âœ“ Belady's Anomaly
âœ“ Memory management concepts (paging, segmentation, TLB)
âœ“ Dependencies and hazards (RAW, WAR, WAW)
âœ“ Cache memory and locality
âœ“ File systems (Unix inodes, hard links)
âœ“ Virtual memory (demand paging, thrashing)

**Study Strategy:**
- Master the definitions - they're easy marks
- Practice page replacement until automatic
- Understand the "why" behind architectural decisions
- Compare and contrast concepts (paging vs segmentation, page fault vs segfault)
- Draw diagrams to visualize concepts

Good luck! ðŸ“šâœ¨
